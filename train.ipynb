{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyORRYCCGHxKpW+p0pXoKnoK"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Download some COCO Data"
      ],
      "metadata": {
        "id": "OlxgCoMVB3rD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")\n",
        "#%cd 'drive/MyDrive/fiftyone'\n",
        "%ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R00AvOEkcSC6",
        "outputId": "bfb74772-3732-4084-8c80-2a833321970d"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "\u001b[0m\u001b[01;34mdatasets\u001b[0m/  \u001b[01;34mdrive\u001b[0m/  \u001b[01;34msample_data\u001b[0m/  \u001b[01;34multralytics\u001b[0m/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd '../datasets/detector-recipe'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EfGF5Nqe26rE",
        "outputId": "1351cdad-a1e7-403a-ec4c-69eaffdd2a93"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/datasets/detector-recipe\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install fiftyone"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9g-Cpmsm34fu",
        "outputId": "627b2307-7bc3-47ab-ba9b-2bf9ebabe31d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: fiftyone in /usr/local/lib/python3.10/dist-packages (0.22.3)\n",
            "Requirement already satisfied: aiofiles in /usr/local/lib/python3.10/dist-packages (from fiftyone) (23.2.1)\n",
            "Requirement already satisfied: argcomplete in /usr/local/lib/python3.10/dist-packages (from fiftyone) (3.1.6)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from fiftyone) (4.11.2)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.10/dist-packages (from fiftyone) (1.33.0)\n",
            "Requirement already satisfied: cachetools in /usr/local/lib/python3.10/dist-packages (from fiftyone) (5.3.2)\n",
            "Requirement already satisfied: dacite<1.8.0,>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from fiftyone) (1.7.0)\n",
            "Requirement already satisfied: Deprecated in /usr/local/lib/python3.10/dist-packages (from fiftyone) (1.2.14)\n",
            "Requirement already satisfied: ftfy in /usr/local/lib/python3.10/dist-packages (from fiftyone) (6.1.3)\n",
            "Requirement already satisfied: humanize in /usr/local/lib/python3.10/dist-packages (from fiftyone) (4.7.0)\n",
            "Requirement already satisfied: hypercorn>=0.13.2 in /usr/local/lib/python3.10/dist-packages (from fiftyone) (0.15.0)\n",
            "Requirement already satisfied: Jinja2>=3 in /usr/local/lib/python3.10/dist-packages (from fiftyone) (3.1.2)\n",
            "Requirement already satisfied: kaleido!=0.2.1.post1 in /usr/local/lib/python3.10/dist-packages (from fiftyone) (0.2.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from fiftyone) (3.7.1)\n",
            "Requirement already satisfied: mongoengine==0.24.2 in /usr/local/lib/python3.10/dist-packages (from fiftyone) (0.24.2)\n",
            "Requirement already satisfied: motor>=2.5 in /usr/local/lib/python3.10/dist-packages (from fiftyone) (3.3.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from fiftyone) (1.23.5)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from fiftyone) (23.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from fiftyone) (1.5.3)\n",
            "Requirement already satisfied: Pillow>=6.2 in /usr/local/lib/python3.10/dist-packages (from fiftyone) (9.4.0)\n",
            "Requirement already satisfied: plotly>=4.14 in /usr/local/lib/python3.10/dist-packages (from fiftyone) (5.15.0)\n",
            "Requirement already satisfied: pprintpp in /usr/local/lib/python3.10/dist-packages (from fiftyone) (0.4.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from fiftyone) (5.9.5)\n",
            "Requirement already satisfied: pymongo>=3.12 in /usr/local/lib/python3.10/dist-packages (from fiftyone) (4.6.0)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.10/dist-packages (from fiftyone) (2023.3.post1)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from fiftyone) (6.0.1)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from fiftyone) (2023.6.3)\n",
            "Requirement already satisfied: retrying in /usr/local/lib/python3.10/dist-packages (from fiftyone) (1.3.4)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from fiftyone) (1.2.2)\n",
            "Requirement already satisfied: scikit-image in /usr/local/lib/python3.10/dist-packages (from fiftyone) (0.19.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from fiftyone) (67.7.2)\n",
            "Requirement already satisfied: sseclient-py<2,>=1.7.2 in /usr/local/lib/python3.10/dist-packages (from fiftyone) (1.8.0)\n",
            "Requirement already satisfied: sse-starlette<1,>=0.10.3 in /usr/local/lib/python3.10/dist-packages (from fiftyone) (0.10.3)\n",
            "Requirement already satisfied: starlette>=0.24.0 in /usr/local/lib/python3.10/dist-packages (from fiftyone) (0.32.0.post1)\n",
            "Requirement already satisfied: strawberry-graphql==0.138.1 in /usr/local/lib/python3.10/dist-packages (from fiftyone) (0.138.1)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.10/dist-packages (from fiftyone) (0.9.0)\n",
            "Requirement already satisfied: xmltodict in /usr/local/lib/python3.10/dist-packages (from fiftyone) (0.13.0)\n",
            "Requirement already satisfied: universal-analytics-python3<2,>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from fiftyone) (1.1.1)\n",
            "Requirement already satisfied: fiftyone-brain~=0.13.2 in /usr/local/lib/python3.10/dist-packages (from fiftyone) (0.13.4)\n",
            "Requirement already satisfied: fiftyone-db~=0.4 in /usr/local/lib/python3.10/dist-packages (from fiftyone) (0.4.0)\n",
            "Requirement already satisfied: voxel51-eta~=0.12 in /usr/local/lib/python3.10/dist-packages (from fiftyone) (0.12.0)\n",
            "Requirement already satisfied: opencv-python-headless in /usr/local/lib/python3.10/dist-packages (from fiftyone) (4.8.1.78)\n",
            "Requirement already satisfied: graphql-core<3.3.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from strawberry-graphql==0.138.1->fiftyone) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.7.0 in /usr/local/lib/python3.10/dist-packages (from strawberry-graphql==0.138.1->fiftyone) (2.8.2)\n",
            "Requirement already satisfied: typing_extensions<5.0.0,>=3.7.4 in /usr/local/lib/python3.10/dist-packages (from strawberry-graphql==0.138.1->fiftyone) (4.5.0)\n",
            "Requirement already satisfied: scipy>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from fiftyone-brain~=0.13.2->fiftyone) (1.11.3)\n",
            "Requirement already satisfied: h11 in /usr/local/lib/python3.10/dist-packages (from hypercorn>=0.13.2->fiftyone) (0.14.0)\n",
            "Requirement already satisfied: h2>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from hypercorn>=0.13.2->fiftyone) (4.1.0)\n",
            "Requirement already satisfied: priority in /usr/local/lib/python3.10/dist-packages (from hypercorn>=0.13.2->fiftyone) (2.0.0)\n",
            "Requirement already satisfied: taskgroup in /usr/local/lib/python3.10/dist-packages (from hypercorn>=0.13.2->fiftyone) (0.0.0a4)\n",
            "Requirement already satisfied: tomli in /usr/local/lib/python3.10/dist-packages (from hypercorn>=0.13.2->fiftyone) (2.0.1)\n",
            "Requirement already satisfied: wsproto>=0.14.0 in /usr/local/lib/python3.10/dist-packages (from hypercorn>=0.13.2->fiftyone) (1.2.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from Jinja2>=3->fiftyone) (2.1.3)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from plotly>=4.14->fiftyone) (8.2.3)\n",
            "Requirement already satisfied: dnspython<3.0.0,>=1.16.0 in /usr/local/lib/python3.10/dist-packages (from pymongo>=3.12->fiftyone) (2.4.2)\n",
            "Requirement already satisfied: anyio<5,>=3.4.0 in /usr/local/lib/python3.10/dist-packages (from starlette>=0.24.0->fiftyone) (3.7.1)\n",
            "Requirement already satisfied: httpx>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from universal-analytics-python3<2,>=1.0.1->fiftyone) (0.25.2)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.10/dist-packages (from voxel51-eta~=0.12->fiftyone) (0.3.7)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.10/dist-packages (from voxel51-eta~=0.12->fiftyone) (0.18.3)\n",
            "Requirement already satisfied: glob2 in /usr/local/lib/python3.10/dist-packages (from voxel51-eta~=0.12->fiftyone) (0.7)\n",
            "Requirement already satisfied: jsonlines in /usr/local/lib/python3.10/dist-packages (from voxel51-eta~=0.12->fiftyone) (4.0.0)\n",
            "Requirement already satisfied: py7zr in /usr/local/lib/python3.10/dist-packages (from voxel51-eta~=0.12->fiftyone) (0.20.8)\n",
            "Requirement already satisfied: rarfile in /usr/local/lib/python3.10/dist-packages (from voxel51-eta~=0.12->fiftyone) (4.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from voxel51-eta~=0.12->fiftyone) (2.31.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from voxel51-eta~=0.12->fiftyone) (1.16.0)\n",
            "Requirement already satisfied: sortedcontainers in /usr/local/lib/python3.10/dist-packages (from voxel51-eta~=0.12->fiftyone) (2.4.0)\n",
            "Requirement already satisfied: tzlocal in /usr/local/lib/python3.10/dist-packages (from voxel51-eta~=0.12->fiftyone) (5.2)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.10/dist-packages (from voxel51-eta~=0.12->fiftyone) (2.0.7)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->fiftyone) (2.5)\n",
            "Requirement already satisfied: botocore<1.34.0,>=1.33.0 in /usr/local/lib/python3.10/dist-packages (from boto3->fiftyone) (1.33.0)\n",
            "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from boto3->fiftyone) (1.0.1)\n",
            "Requirement already satisfied: s3transfer<0.9.0,>=0.8.0 in /usr/local/lib/python3.10/dist-packages (from boto3->fiftyone) (0.8.0)\n",
            "Requirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.10/dist-packages (from Deprecated->fiftyone) (1.14.1)\n",
            "Requirement already satisfied: wcwidth<0.3.0,>=0.2.12 in /usr/local/lib/python3.10/dist-packages (from ftfy->fiftyone) (0.2.12)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->fiftyone) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->fiftyone) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->fiftyone) (4.44.3)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->fiftyone) (1.4.5)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->fiftyone) (3.1.1)\n",
            "Requirement already satisfied: networkx>=2.2 in /usr/local/lib/python3.10/dist-packages (from scikit-image->fiftyone) (3.2.1)\n",
            "Requirement already satisfied: imageio>=2.4.1 in /usr/local/lib/python3.10/dist-packages (from scikit-image->fiftyone) (2.31.6)\n",
            "Requirement already satisfied: tifffile>=2019.7.26 in /usr/local/lib/python3.10/dist-packages (from scikit-image->fiftyone) (2023.9.26)\n",
            "Requirement already satisfied: PyWavelets>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-image->fiftyone) (1.4.1)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->fiftyone) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->fiftyone) (3.2.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.4.0->starlette>=0.24.0->fiftyone) (3.4)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.4.0->starlette>=0.24.0->fiftyone) (1.3.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.4.0->starlette>=0.24.0->fiftyone) (1.1.3)\n",
            "Requirement already satisfied: hyperframe<7,>=6.0 in /usr/local/lib/python3.10/dist-packages (from h2>=3.1.0->hypercorn>=0.13.2->fiftyone) (6.0.1)\n",
            "Requirement already satisfied: hpack<5,>=4.0 in /usr/local/lib/python3.10/dist-packages (from h2>=3.1.0->hypercorn>=0.13.2->fiftyone) (4.0.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx>=0.10.0->universal-analytics-python3<2,>=1.0.1->fiftyone) (2023.7.22)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx>=0.10.0->universal-analytics-python3<2,>=1.0.1->fiftyone) (1.0.2)\n",
            "Requirement already satisfied: attrs>=19.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonlines->voxel51-eta~=0.12->fiftyone) (23.1.0)\n",
            "Requirement already satisfied: texttable in /usr/local/lib/python3.10/dist-packages (from py7zr->voxel51-eta~=0.12->fiftyone) (1.7.0)\n",
            "Requirement already satisfied: pycryptodomex>=3.16.0 in /usr/local/lib/python3.10/dist-packages (from py7zr->voxel51-eta~=0.12->fiftyone) (3.19.0)\n",
            "Requirement already satisfied: pyzstd>=0.15.9 in /usr/local/lib/python3.10/dist-packages (from py7zr->voxel51-eta~=0.12->fiftyone) (0.15.9)\n",
            "Requirement already satisfied: pyppmd<1.2.0,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from py7zr->voxel51-eta~=0.12->fiftyone) (1.1.0)\n",
            "Requirement already satisfied: pybcj<1.1.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from py7zr->voxel51-eta~=0.12->fiftyone) (1.0.2)\n",
            "Requirement already satisfied: multivolumefile>=0.2.3 in /usr/local/lib/python3.10/dist-packages (from py7zr->voxel51-eta~=0.12->fiftyone) (0.2.3)\n",
            "Requirement already satisfied: inflate64<1.1.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from py7zr->voxel51-eta~=0.12->fiftyone) (1.0.0)\n",
            "Requirement already satisfied: brotli>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from py7zr->voxel51-eta~=0.12->fiftyone) (1.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->voxel51-eta~=0.12->fiftyone) (3.3.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install fiftyone-db-ubuntu2204"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RtR8cSJh3-YT",
        "outputId": "ab01701d-afe0-4d0b-c7e3-c3880dc60806"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting fiftyone-db-ubuntu2204\n",
            "  Downloading fiftyone_db_ubuntu2204-0.4.0-py3-none-manylinux1_x86_64.whl (42.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.2/42.2 MB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: fiftyone-db-ubuntu2204\n",
            "Successfully installed fiftyone-db-ubuntu2204-0.4.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import fiftyone as fo\n",
        "import fiftyone.zoo as foz\n",
        "\n",
        "dataset = foz.load_zoo_dataset(\n",
        "    \"coco-2017\",\n",
        "    splits=[\"validation\",\"train\"],\n",
        "    classes=[\"person\", \"car\"],\n",
        "    dataset_name=\"detector-recipe\",\n",
        "    max_samples=10000,\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xRvn_ZFf6Oe1",
        "outputId": "d15a9642-8a25-4084-fd12-ac3c253906fb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading split 'validation' to '/root/fiftyone/coco-2017/validation' if necessary\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:fiftyone.zoo.datasets:Downloading split 'validation' to '/root/fiftyone/coco-2017/validation' if necessary\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found annotations at '/root/fiftyone/coco-2017/raw/instances_val2017.json'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:fiftyone.utils.coco:Found annotations at '/root/fiftyone/coco-2017/raw/instances_val2017.json'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Only found 2869 (<10000) samples matching your requirements\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:fiftyone.utils.coco:Only found 2869 (<10000) samples matching your requirements\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sufficient images already downloaded\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:fiftyone.utils.coco:Sufficient images already downloaded\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Existing download of split 'validation' is sufficient\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:fiftyone.zoo.datasets:Existing download of split 'validation' is sufficient\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading split 'train' to '/root/fiftyone/coco-2017/train' if necessary\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:fiftyone.zoo.datasets:Downloading split 'train' to '/root/fiftyone/coco-2017/train' if necessary\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found annotations at '/root/fiftyone/coco-2017/raw/instances_train2017.json'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:fiftyone.utils.coco:Found annotations at '/root/fiftyone/coco-2017/raw/instances_train2017.json'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading 10000 images\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:fiftyone.utils.coco:Downloading 10000 images\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " 100% |██████████████| 10000/10000 [36.9m elapsed, 0s remaining, 4.4 images/s]      \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:eta.core.utils: 100% |██████████████| 10000/10000 [36.9m elapsed, 0s remaining, 4.4 images/s]      \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing annotations for 10000 downloaded samples to '/root/fiftyone/coco-2017/train/labels.json'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:fiftyone.utils.coco:Writing annotations for 10000 downloaded samples to '/root/fiftyone/coco-2017/train/labels.json'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset info written to '/root/fiftyone/coco-2017/info.json'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:fiftyone.zoo.datasets:Dataset info written to '/root/fiftyone/coco-2017/info.json'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading existing dataset 'detector-recipe'. To reload from disk, either delete the existing dataset or provide a custom `dataset_name` to use\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:fiftyone.zoo.datasets:Loading existing dataset 'detector-recipe'. To reload from disk, either delete the existing dataset or provide a custom `dataset_name` to use\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qGMHZe0i8DsQ",
        "outputId": "b8ec09b7-ae53-4c42-8bd8-89379c540750"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Name:        detector-recipe\n",
              "Media type:  image\n",
              "Num samples: 5000\n",
              "Persistent:  False\n",
              "Tags:        []\n",
              "Sample fields:\n",
              "    id:           fiftyone.core.fields.ObjectIdField\n",
              "    filepath:     fiftyone.core.fields.StringField\n",
              "    tags:         fiftyone.core.fields.ListField(fiftyone.core.fields.StringField)\n",
              "    metadata:     fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.metadata.ImageMetadata)\n",
              "    ground_truth: fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.labels.Detections)"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_joGOkUh_CSJ",
        "outputId": "3d54c24b-f5a5-4761-ff39-73ae710f4456"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[0m\u001b[01;34mcoco-2017\u001b[0m/  info.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load in the data"
      ],
      "metadata": {
        "id": "kvvDvyNhCG-I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")\n",
        "%cd 'drive/MyDrive/datasets/detector-recipe'\n",
        "%ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7OJYAUQmDG6K",
        "outputId": "930b83f4-02b8-4f28-8842-77be3e58526d"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "/content/drive/MyDrive/datasets/detector-recipe\n",
            "\u001b[0m\u001b[01;34mcoco-2017\u001b[0m/  info.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd detector-recipe/\n",
        "%ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BuV2cQSjDut-",
        "outputId": "0da39d36-4c6d-4cd8-8f60-875c5fe621b9"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Errno 2] No such file or directory: 'detector-recipe/'\n",
            "/content/drive/MyDrive/datasets/detector-recipe\n",
            "\u001b[0m\u001b[01;34mcoco-2017\u001b[0m/  info.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "APRxtH8Db8P_",
        "outputId": "e1d122c8-82c6-4ae2-dc04-4e9cc31c2408"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loading annotations into memory...\n",
            "Done (t=4.13s)\n",
            "creating index...\n",
            "index created!\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from torchvision.datasets import CocoDetection\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision.transforms import PILToTensor, Resize, Lambda, v2\n",
        "\n",
        "preproc = v2.Compose([v2.PILToTensor(),\n",
        "                      v2.Resize((640,640))])\n",
        "\n",
        "\n",
        "def output_transform(output):\n",
        "  t = torch.zeros([1,7,100])\n",
        "  t[:, 4, :] = torch.tensor([1]*100).unsqueeze(dim=0) # initialize everything to background\n",
        "  for idx, det in enumerate(output):\n",
        "    if det['category_id'] == 1: #person is 1\n",
        "      t[:, :, idx] = torch.concat((torch.tensor(det['bbox']), torch.tensor([0,1,0]))).unsqueeze(dim=0)\n",
        "    if det['category_id'] == 3: #car is 3\n",
        "      t[:, :, idx] = torch.concat((torch.tensor(det['bbox']), torch.tensor([0,0,1]))).unsqueeze(dim=0)\n",
        "  return t[0]\n",
        "\n",
        "\n",
        "dataset = CocoDetection(root='./coco-2017/train/data', annFile='./coco-2017/train/labels.json', transform=preproc, target_transform=output_transform)\n",
        "train_loader = DataLoader(dataset)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class litedetect(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        # Layer 1\n",
        "        self.a = Conv(in_channels=3, out_channels=8, kernel_size=1, stride=2) # down to 320x320x8\n",
        "        self.b = Conv(in_channels=8, out_channels=16, kernel_size=1, stride=2) # down to 160x160x16\n",
        "        self.c = Conv(in_channels=16, out_channels=32, kernel_size=1, stride=2) # down to 80x80x32\n",
        "        self.d = Conv(in_channels=32, out_channels=64, kernel_size=1, stride=2) # down to 40x40x64\n",
        "        self.e = Conv(in_channels=64, out_channels=64, kernel_size=1, stride=2) # down to 20x20x64\n",
        "        self.f = torch.nn.MaxPool2d(kernel_size=(2,2), stride=(1,1), padding=(1,1)) # still 20x20x64\n",
        "\n",
        "        # Layer 2\n",
        "        self.f_up = torch.nn.Upsample(size=(40,40)) # 40x40x64\n",
        "        self.d_l2 = Conv(in_channels=128, out_channels=128, kernel_size=1, stride=1)\n",
        "        self.d_l2_up = torch.nn.Upsample(size=(80,80)) # 80x80x128\n",
        "        self.c_l2 = Conv(in_channels=160, out_channels=160, kernel_size=1, stride=1)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Layer 1\n",
        "        a_out = self.a(x)\n",
        "        b_out = self.b(a_out)\n",
        "        c_out = self.c(b_out)\n",
        "        d_out = self.d(c_out)\n",
        "        e_out = self.e(d_out)\n",
        "        f_out = self.f(e_out)\n",
        "\n",
        "        # Layer 2\n",
        "        f_up_out = self.f_up(f_out) # 40x40x64\n",
        "        f_d_cat = torch.concat(f_up_out, d_out) # 40x40x128\n",
        "        d_l2_out = self.d_l2(f_d_cat)\n",
        "        d_l2_up_out = self.d_l2_up(d_l2_out) # 80x80x128\n",
        "        d_c_cat = torch.concat(d_l2_up_out, c_out) # 80x80x160\n",
        "        c_l2_out = self.c_l2(d_c_cat)\n",
        "\n",
        "\n",
        "class Conv(torch.nn.Module):\n",
        "    def  __init__(self,in_channels, out_channels, kernel_size, stride, padding):\n",
        "        super().__init__()\n",
        "        self.a = torch.nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=kernel_size,\n",
        "                                 stride=stride, padding=padding)\n",
        "        self.b = torch.nn.BatchNorm2d(num_features=out_channels)\n",
        "        self.c = torch.nn.SiLU()\n",
        "\n",
        "    def forward(self,x):\n",
        "        a_out = self.a(x)\n",
        "        b_out = self.b(a_out)\n",
        "        return self.c(b_out"
      ],
      "metadata": {
        "id": "5ZpJ_u06gUno"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for data, target in train_loader:\n",
        "  print(data)\n",
        "  print(target)\n",
        "  break"
      ],
      "metadata": {
        "id": "6Q3_t4BjgZ9t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "target.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SaBDgjX3W1lo",
        "outputId": "2e932234-ca46-498f-9e4f-47de3591f637"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 7, 100])"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "t = torch.zeros([1,7,100])\n",
        "t[:, 4, :] = torch.tensor([1]*100).unsqueeze(dim=0)\n",
        "t[:, :, 1] = torch.concat((torch.tensor(target[1]['bbox']), torch.tensor([0,1,0]))).unsqueeze(0)\n",
        "t"
      ],
      "metadata": {
        "id": "tAKhgvmOXbcK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "f9tnQ5gwNBbv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Work with YOLOV8 for Loss Function"
      ],
      "metadata": {
        "id": "FWxBFJDs-wZ2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def box_iou(box1, box2, eps=1e-7):\n",
        "    \"\"\"\n",
        "    Calculate intersection-over-union (IoU) of boxes. Both sets of boxes are expected to be in (x1, y1, x2, y2) format.\n",
        "    Based on https://github.com/pytorch/vision/blob/master/torchvision/ops/boxes.py\n",
        "\n",
        "    Args:\n",
        "        box1 (torch.Tensor): A tensor of shape (N, 4) representing N bounding boxes.\n",
        "        box2 (torch.Tensor): A tensor of shape (M, 4) representing M bounding boxes.\n",
        "        eps (float, optional): A small value to avoid division by zero. Defaults to 1e-7.\n",
        "\n",
        "    Returns:\n",
        "        (torch.Tensor): An NxM tensor containing the pairwise IoU values for every element in box1 and box2.\n",
        "    \"\"\"\n",
        "\n",
        "    # inter(N,M) = (rb(N,M,2) - lt(N,M,2)).clamp(0).prod(2)\n",
        "    (a1, a2), (b1, b2) = box1.unsqueeze(1).chunk(2, 2), box2.unsqueeze(0).chunk(2, 2)\n",
        "    inter = (torch.min(a2, b2) - torch.max(a1, b1)).clamp_(0).prod(2)\n",
        "\n",
        "    # IoU = inter / (area1 + area2 - inter)\n",
        "    return inter / ((a2 - a1).prod(2) + (b2 - b1).prod(2) - inter + eps)"
      ],
      "metadata": {
        "id": "XiSMiCwK0Gav"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "class TaskAlignedAssigner(nn.Module):\n",
        "    \"\"\"\n",
        "    A task-aligned assigner for object detection.\n",
        "\n",
        "    This class assigns ground-truth (gt) objects to anchors based on the task-aligned metric, which combines both\n",
        "    classification and localization information.\n",
        "\n",
        "    Attributes:\n",
        "        topk (int): The number of top candidates to consider.\n",
        "        num_classes (int): The number of object classes.\n",
        "        alpha (float): The alpha parameter for the classification component of the task-aligned metric.\n",
        "        beta (float): The beta parameter for the localization component of the task-aligned metric.\n",
        "        eps (float): A small value to prevent division by zero.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, topk=13, num_classes=80, alpha=1.0, beta=6.0, eps=1e-9):\n",
        "        \"\"\"Initialize a TaskAlignedAssigner object with customizable hyperparameters.\"\"\"\n",
        "        super().__init__()\n",
        "        self.topk = topk\n",
        "        self.num_classes = num_classes\n",
        "        self.bg_idx = num_classes\n",
        "        self.alpha = alpha\n",
        "        self.beta = beta\n",
        "        self.eps = eps\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def forward(self, pd_scores, pd_bboxes, anc_points, gt_labels, gt_bboxes, mask_gt):\n",
        "        \"\"\"\n",
        "        Compute the task-aligned assignment. Reference code is available at\n",
        "        https://github.com/Nioolek/PPYOLOE_pytorch/blob/master/ppyoloe/assigner/tal_assigner.py.\n",
        "\n",
        "        Args:\n",
        "            pd_scores (Tensor): shape(bs, num_total_anchors, num_classes)\n",
        "            pd_bboxes (Tensor): shape(bs, num_total_anchors, 4)\n",
        "            anc_points (Tensor): shape(num_total_anchors, 2)\n",
        "            gt_labels (Tensor): shape(bs, n_max_boxes, 1)\n",
        "            gt_bboxes (Tensor): shape(bs, n_max_boxes, 4)\n",
        "            mask_gt (Tensor): shape(bs, n_max_boxes, 1)\n",
        "\n",
        "        Returns:\n",
        "            target_labels (Tensor): shape(bs, num_total_anchors)\n",
        "            target_bboxes (Tensor): shape(bs, num_total_anchors, 4)\n",
        "            target_scores (Tensor): shape(bs, num_total_anchors, num_classes)\n",
        "            fg_mask (Tensor): shape(bs, num_total_anchors)\n",
        "            target_gt_idx (Tensor): shape(bs, num_total_anchors)\n",
        "        \"\"\"\n",
        "        self.bs = pd_scores.size(0)\n",
        "        self.n_max_boxes = gt_bboxes.size(1)\n",
        "\n",
        "        if self.n_max_boxes == 0:\n",
        "            device = gt_bboxes.device\n",
        "            return (torch.full_like(pd_scores[..., 0], self.bg_idx).to(device), torch.zeros_like(pd_bboxes).to(device),\n",
        "                    torch.zeros_like(pd_scores).to(device), torch.zeros_like(pd_scores[..., 0]).to(device),\n",
        "                    torch.zeros_like(pd_scores[..., 0]).to(device))\n",
        "\n",
        "        mask_pos, align_metric, overlaps = self.get_pos_mask(pd_scores, pd_bboxes, gt_labels, gt_bboxes, anc_points,\n",
        "                                                             mask_gt)\n",
        "\n",
        "        target_gt_idx, fg_mask, mask_pos = select_highest_overlaps(mask_pos, overlaps, self.n_max_boxes)\n",
        "\n",
        "        # Assigned target\n",
        "        target_labels, target_bboxes, target_scores = self.get_targets(gt_labels, gt_bboxes, target_gt_idx, fg_mask)\n",
        "\n",
        "        # Normalize\n",
        "        align_metric *= mask_pos\n",
        "        pos_align_metrics = align_metric.amax(dim=-1, keepdim=True)  # b, max_num_obj\n",
        "        pos_overlaps = (overlaps * mask_pos).amax(dim=-1, keepdim=True)  # b, max_num_obj\n",
        "        norm_align_metric = (align_metric * pos_overlaps / (pos_align_metrics + self.eps)).amax(-2).unsqueeze(-1)\n",
        "        target_scores = target_scores * norm_align_metric\n",
        "\n",
        "        return target_labels, target_bboxes, target_scores, fg_mask.bool(), target_gt_idx\n",
        "\n",
        "    def get_pos_mask(self, pd_scores, pd_bboxes, gt_labels, gt_bboxes, anc_points, mask_gt):\n",
        "        \"\"\"Get in_gts mask, (b, max_num_obj, h*w).\"\"\"\n",
        "        mask_in_gts = select_candidates_in_gts(anc_points, gt_bboxes)\n",
        "        # Get anchor_align metric, (b, max_num_obj, h*w)\n",
        "        align_metric, overlaps = self.get_box_metrics(pd_scores, pd_bboxes, gt_labels, gt_bboxes, mask_in_gts * mask_gt)\n",
        "        # Get topk_metric mask, (b, max_num_obj, h*w)\n",
        "        mask_topk = self.select_topk_candidates(align_metric, topk_mask=mask_gt.expand(-1, -1, self.topk).bool())\n",
        "        # Merge all mask to a final mask, (b, max_num_obj, h*w)\n",
        "        mask_pos = mask_topk * mask_in_gts * mask_gt\n",
        "\n",
        "        return mask_pos, align_metric, overlaps\n",
        "\n",
        "    def get_box_metrics(self, pd_scores, pd_bboxes, gt_labels, gt_bboxes, mask_gt):\n",
        "        \"\"\"Compute alignment metric given predicted and ground truth bounding boxes.\"\"\"\n",
        "        na = pd_bboxes.shape[-2]\n",
        "        mask_gt = mask_gt.bool()  # b, max_num_obj, h*w\n",
        "        overlaps = torch.zeros([self.bs, self.n_max_boxes, na], dtype=pd_bboxes.dtype, device=pd_bboxes.device)\n",
        "        bbox_scores = torch.zeros([self.bs, self.n_max_boxes, na], dtype=pd_scores.dtype, device=pd_scores.device)\n",
        "\n",
        "        ind = torch.zeros([2, self.bs, self.n_max_boxes], dtype=torch.long)  # 2, b, max_num_obj\n",
        "        ind[0] = torch.arange(end=self.bs).view(-1, 1).expand(-1, self.n_max_boxes)  # b, max_num_obj\n",
        "        ind[1] = gt_labels.squeeze(-1)  # b, max_num_obj\n",
        "        # Get the scores of each grid for each gt cls\n",
        "        bbox_scores[mask_gt] = pd_scores[ind[0], :, ind[1]][mask_gt]  # b, max_num_obj, h*w\n",
        "\n",
        "        # (b, max_num_obj, 1, 4), (b, 1, h*w, 4)\n",
        "        pd_boxes = pd_bboxes.unsqueeze(1).expand(-1, self.n_max_boxes, -1, -1)[mask_gt]\n",
        "        gt_boxes = gt_bboxes.unsqueeze(2).expand(-1, -1, na, -1)[mask_gt]\n",
        "        overlaps[mask_gt] = bbox_iou(gt_boxes, pd_boxes, xywh=False, CIoU=True).squeeze(-1).clamp_(0)\n",
        "\n",
        "        align_metric = bbox_scores.pow(self.alpha) * overlaps.pow(self.beta)\n",
        "        return align_metric, overlaps\n",
        "\n",
        "    def select_topk_candidates(self, metrics, largest=True, topk_mask=None):\n",
        "        \"\"\"\n",
        "        Select the top-k candidates based on the given metrics.\n",
        "\n",
        "        Args:\n",
        "            metrics (Tensor): A tensor of shape (b, max_num_obj, h*w), where b is the batch size,\n",
        "                              max_num_obj is the maximum number of objects, and h*w represents the\n",
        "                              total number of anchor points.\n",
        "            largest (bool): If True, select the largest values; otherwise, select the smallest values.\n",
        "            topk_mask (Tensor): An optional boolean tensor of shape (b, max_num_obj, topk), where\n",
        "                                topk is the number of top candidates to consider. If not provided,\n",
        "                                the top-k values are automatically computed based on the given metrics.\n",
        "\n",
        "        Returns:\n",
        "            (Tensor): A tensor of shape (b, max_num_obj, h*w) containing the selected top-k candidates.\n",
        "        \"\"\"\n",
        "\n",
        "        # (b, max_num_obj, topk)\n",
        "        topk_metrics, topk_idxs = torch.topk(metrics, self.topk, dim=-1, largest=largest)\n",
        "        if topk_mask is None:\n",
        "            topk_mask = (topk_metrics.max(-1, keepdim=True)[0] > self.eps).expand_as(topk_idxs)\n",
        "        # (b, max_num_obj, topk)\n",
        "        topk_idxs.masked_fill_(~topk_mask, 0)\n",
        "\n",
        "        # (b, max_num_obj, topk, h*w) -> (b, max_num_obj, h*w)\n",
        "        count_tensor = torch.zeros(metrics.shape, dtype=torch.int8, device=topk_idxs.device)\n",
        "        ones = torch.ones_like(topk_idxs[:, :, :1], dtype=torch.int8, device=topk_idxs.device)\n",
        "        for k in range(self.topk):\n",
        "            # Expand topk_idxs for each value of k and add 1 at the specified positions\n",
        "            count_tensor.scatter_add_(-1, topk_idxs[:, :, k:k + 1], ones)\n",
        "        # count_tensor.scatter_add_(-1, topk_idxs, torch.ones_like(topk_idxs, dtype=torch.int8, device=topk_idxs.device))\n",
        "        # Filter invalid bboxes\n",
        "        count_tensor.masked_fill_(count_tensor > 1, 0)\n",
        "\n",
        "        return count_tensor.to(metrics.dtype)\n",
        "\n",
        "    def get_targets(self, gt_labels, gt_bboxes, target_gt_idx, fg_mask):\n",
        "        \"\"\"\n",
        "        Compute target labels, target bounding boxes, and target scores for the positive anchor points.\n",
        "\n",
        "        Args:\n",
        "            gt_labels (Tensor): Ground truth labels of shape (b, max_num_obj, 1), where b is the\n",
        "                                batch size and max_num_obj is the maximum number of objects.\n",
        "            gt_bboxes (Tensor): Ground truth bounding boxes of shape (b, max_num_obj, 4).\n",
        "            target_gt_idx (Tensor): Indices of the assigned ground truth objects for positive\n",
        "                                    anchor points, with shape (b, h*w), where h*w is the total\n",
        "                                    number of anchor points.\n",
        "            fg_mask (Tensor): A boolean tensor of shape (b, h*w) indicating the positive\n",
        "                              (foreground) anchor points.\n",
        "\n",
        "        Returns:\n",
        "            (Tuple[Tensor, Tensor, Tensor]): A tuple containing the following tensors:\n",
        "                - target_labels (Tensor): Shape (b, h*w), containing the target labels for\n",
        "                                          positive anchor points.\n",
        "                - target_bboxes (Tensor): Shape (b, h*w, 4), containing the target bounding boxes\n",
        "                                          for positive anchor points.\n",
        "                - target_scores (Tensor): Shape (b, h*w, num_classes), containing the target scores\n",
        "                                          for positive anchor points, where num_classes is the number\n",
        "                                          of object classes.\n",
        "        \"\"\"\n",
        "\n",
        "        # Assigned target labels, (b, 1)\n",
        "        batch_ind = torch.arange(end=self.bs, dtype=torch.int64, device=gt_labels.device)[..., None]\n",
        "        target_gt_idx = target_gt_idx + batch_ind * self.n_max_boxes  # (b, h*w)\n",
        "        target_labels = gt_labels.long().flatten()[target_gt_idx]  # (b, h*w)\n",
        "\n",
        "        # Assigned target boxes, (b, max_num_obj, 4) -> (b, h*w, 4)\n",
        "        target_bboxes = gt_bboxes.view(-1, 4)[target_gt_idx]\n",
        "\n",
        "        # Assigned target scores\n",
        "        target_labels.clamp_(0)\n",
        "\n",
        "        # 10x faster than F.one_hot()\n",
        "        target_scores = torch.zeros((target_labels.shape[0], target_labels.shape[1], self.num_classes),\n",
        "                                    dtype=torch.int64,\n",
        "                                    device=target_labels.device)  # (b, h*w, 80)\n",
        "        target_scores.scatter_(2, target_labels.unsqueeze(-1), 1)\n",
        "\n",
        "        fg_scores_mask = fg_mask[:, :, None].repeat(1, 1, self.num_classes)  # (b, h*w, 80)\n",
        "        target_scores = torch.where(fg_scores_mask > 0, target_scores, 0)\n",
        "\n",
        "        return target_labels, target_bboxes, target_scores"
      ],
      "metadata": {
        "id": "KcuBGDf-zi50"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class v8DetectionLoss:\n",
        "    \"\"\"Criterion class for computing training losses.\"\"\"\n",
        "\n",
        "    def __init__(self, model):  # model must be de-paralleled\n",
        "        \"\"\"Initializes v8DetectionLoss with the model, defining model-related properties and BCE loss function.\"\"\"\n",
        "        device = next(model.parameters()).device  # get model device\n",
        "        h = model.args  # hyperparameters\n",
        "\n",
        "        m = model.model[-1]  # Detect() module\n",
        "        self.bce = nn.BCEWithLogitsLoss(reduction='none')\n",
        "        self.hyp = h\n",
        "        self.stride = m.stride  # model strides\n",
        "        self.nc = m.nc  # number of classes\n",
        "        self.no = m.no\n",
        "        self.reg_max = m.reg_max\n",
        "        self.device = device\n",
        "\n",
        "        self.use_dfl = m.reg_max > 1\n",
        "\n",
        "        self.assigner = TaskAlignedAssigner(topk=10, num_classes=self.nc, alpha=0.5, beta=6.0)\n",
        "        self.bbox_loss = BboxLoss(m.reg_max - 1, use_dfl=self.use_dfl).to(device)\n",
        "        self.proj = torch.arange(m.reg_max, dtype=torch.float, device=device)\n",
        "\n",
        "    def preprocess(self, targets, batch_size, scale_tensor):\n",
        "        \"\"\"Preprocesses the target counts and matches with the input batch size to output a tensor.\"\"\"\n",
        "        if targets.shape[0] == 0:\n",
        "            out = torch.zeros(batch_size, 0, 5, device=self.device)\n",
        "        else:\n",
        "            i = targets[:, 0]  # image index\n",
        "            _, counts = i.unique(return_counts=True)\n",
        "            counts = counts.to(dtype=torch.int32)\n",
        "            out = torch.zeros(batch_size, counts.max(), 5, device=self.device)\n",
        "            for j in range(batch_size):\n",
        "                matches = i == j\n",
        "                n = matches.sum()\n",
        "                if n:\n",
        "                    out[j, :n] = targets[matches, 1:]\n",
        "            out[..., 1:5] = xywh2xyxy(out[..., 1:5].mul_(scale_tensor))\n",
        "        return out\n",
        "\n",
        "    def bbox_decode(self, anchor_points, pred_dist):\n",
        "        \"\"\"Decode predicted object bounding box coordinates from anchor points and distribution.\"\"\"\n",
        "        if self.use_dfl:\n",
        "            b, a, c = pred_dist.shape  # batch, anchors, channels\n",
        "            pred_dist = pred_dist.view(b, a, 4, c // 4).softmax(3).matmul(self.proj.type(pred_dist.dtype))\n",
        "            # pred_dist = pred_dist.view(b, a, c // 4, 4).transpose(2,3).softmax(3).matmul(self.proj.type(pred_dist.dtype))\n",
        "            # pred_dist = (pred_dist.view(b, a, c // 4, 4).softmax(2) * self.proj.type(pred_dist.dtype).view(1, 1, -1, 1)).sum(2)\n",
        "        return dist2bbox(pred_dist, anchor_points, xywh=False)\n",
        "\n",
        "    def __call__(self, preds, batch):\n",
        "        \"\"\"Calculate the sum of the loss for box, cls and dfl multiplied by batch size.\"\"\"\n",
        "        loss = torch.zeros(3, device=self.device)  # box, cls, dfl\n",
        "        feats = preds[1] if isinstance(preds, tuple) else preds\n",
        "        pred_distri, pred_scores = torch.cat([xi.view(feats[0].shape[0], self.no, -1) for xi in feats], 2).split(\n",
        "            (self.reg_max * 4, self.nc), 1)\n",
        "\n",
        "        pred_scores = pred_scores.permute(0, 2, 1).contiguous()\n",
        "        pred_distri = pred_distri.permute(0, 2, 1).contiguous()\n",
        "\n",
        "        dtype = pred_scores.dtype\n",
        "        batch_size = pred_scores.shape[0]\n",
        "        imgsz = torch.tensor(feats[0].shape[2:], device=self.device, dtype=dtype) * self.stride[0]  # image size (h,w)\n",
        "        anchor_points, stride_tensor = make_anchors(feats, self.stride, 0.5)\n",
        "\n",
        "        # Targets\n",
        "        targets = torch.cat((batch['batch_idx'].view(-1, 1), batch['cls'].view(-1, 1), batch['bboxes']), 1)\n",
        "        targets = self.preprocess(targets.to(self.device), batch_size, scale_tensor=imgsz[[1, 0, 1, 0]])\n",
        "        gt_labels, gt_bboxes = targets.split((1, 4), 2)  # cls, xyxy\n",
        "        mask_gt = gt_bboxes.sum(2, keepdim=True).gt_(0)\n",
        "\n",
        "        # Pboxes\n",
        "        pred_bboxes = self.bbox_decode(anchor_points, pred_distri)  # xyxy, (b, h*w, 4)\n",
        "\n",
        "        _, target_bboxes, target_scores, fg_mask, _ = self.assigner(\n",
        "            pred_scores.detach().sigmoid(), (pred_bboxes.detach() * stride_tensor).type(gt_bboxes.dtype),\n",
        "            anchor_points * stride_tensor, gt_labels, gt_bboxes, mask_gt)\n",
        "\n",
        "        target_scores_sum = max(target_scores.sum(), 1)\n",
        "\n",
        "        # Cls loss\n",
        "        # loss[1] = self.varifocal_loss(pred_scores, target_scores, target_labels) / target_scores_sum  # VFL way\n",
        "        loss[1] = self.bce(pred_scores, target_scores.to(dtype)).sum() / target_scores_sum  # BCE\n",
        "\n",
        "        # Bbox loss\n",
        "        if fg_mask.sum():\n",
        "            target_bboxes /= stride_tensor\n",
        "            loss[0], loss[2] = self.bbox_loss(pred_distri, pred_bboxes, anchor_points, target_bboxes, target_scores,\n",
        "                                              target_scores_sum, fg_mask)\n",
        "\n",
        "        loss[0] *= self.hyp.box  # box gain\n",
        "        loss[1] *= self.hyp.cls  # cls gain\n",
        "        loss[2] *= self.hyp.dfl  # dfl gain\n",
        "\n",
        "        return loss.sum() * batch_size, loss.detach()  # loss(box, cls, dfl)"
      ],
      "metadata": {
        "id": "Q7AnDfuDjjwf"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! git clone https://github.com/ultralytics/ultralytics.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Zr8z2Qt-3YS",
        "outputId": "572f17d3-c2e2-411b-992d-9214180d4bf4"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'ultralytics'...\n",
            "remote: Enumerating objects: 20550, done.\u001b[K\n",
            "remote: Counting objects: 100% (1737/1737), done.\u001b[K\n",
            "remote: Compressing objects: 100% (986/986), done.\u001b[K\n",
            "remote: Total 20550 (delta 1056), reused 1185 (delta 747), pack-reused 18813\u001b[K\n",
            "Receiving objects: 100% (20550/20550), 11.44 MiB | 11.71 MiB/s, done.\n",
            "Resolving deltas: 100% (14221/14221), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Run this in a python session in the terminal. cd to /content/ultralytics\n",
        "\n",
        "put a break point in line 203 of loss under utils"
      ],
      "metadata": {
        "id": "LJymLdQ-KW9y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import ultralytics\n",
        "from ultralytics.models.yolo import model\n",
        "my_model = model.YOLO('yolov8.yaml')\n",
        "my_model.train(data=\"coco128.yaml\", epochs=3)"
      ],
      "metadata": {
        "id": "QDegbpUe_bzL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from os import walk\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "images = '/content/datasets/coco128/images/train2017/'\n",
        "labels = '/content/datasets/coco128/labels/train2017/'\n",
        "dir = os.listdir(labels)\n",
        "for file in dir:\n",
        "  print(file)\n",
        "  df = pd.read_csv(labels+file, sep=' ', header=None)\n",
        "  df.loc[:,0].replace(to_replace=[2,5,7], value=1, inplace=True)\n",
        "  df = df[df[0] == (0 or 1)]\n",
        "  if df.shape[0] == 0:\n",
        "    os.remove(labels+file)\n",
        "    try:\n",
        "      os.remove(images+file.strip('.txt')+'.jpg')\n",
        "    except:\n",
        "      continue\n",
        "  else:\n",
        "    df.to_csv(path_or_buf=labels+file, sep=' ', header=False, index=False)"
      ],
      "metadata": {
        "id": "iSrTFqy_HPFV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gfgkbSsyV1GG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}